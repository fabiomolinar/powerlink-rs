// crates/powerlink-rs/src/node/mn/scheduler.rs
use super::cycle; // Added for cycle::advance_cycle_phase
use super::events; // Added for events::handle_dll_event
use super::payload; // Added for payload::build...
use super::state::MnContext;
use crate::common::{NetTime, RelativeTime};
use crate::frame::basic::MacAddress;
use crate::frame::codec::CodecHelpers;
use crate::frame::{
    ASndFrame, DllMsEvent, PowerlinkFrame, RequestedServiceId, ServiceId, SocFrame,
};
use crate::nmt::events::{NmtCommand, NmtEvent};
use crate::nmt::{states::NmtState, NmtStateMachine};
use crate::node::mn::state::{CnInfo, CnState, CyclePhase};
use crate::node::{NodeAction, serialize_frame_action, build_udp_from_sdo_response, build_asnd_from_sdo_response};
use crate::node::NodeContext;
use crate::od::{Object, ObjectValue};
use crate::sdo::asnd;
use crate::sdo::command::SdoCommand; // Added import
use crate::sdo::sequence::SequenceLayerHeader; // Added import
use crate::sdo::server::SdoClientInfo;
#[cfg(feature = "sdo-udp")]
use crate::sdo::udp::serialize_sdo_udp_payload;
use crate::types::{C_ADR_BROADCAST_NODE_ID, C_ADR_MN_DEF_NODE_ID, NodeId};
use crate::PowerlinkError;
use alloc::vec;
use alloc::vec::Vec;
use log::{debug, error, info, trace, warn};

// Constants for OD access, moved from main.rs
const OD_IDX_CYCLE_TIME: u16 = 0x1006;
const OD_IDX_MAC_MAP: u16 = 0x1F84;

/// Determines the highest priority asynchronous action to be taken.
/// The priority is:
// ... (existing code)
pub(super) fn determine_next_async_action(
    context: &mut MnContext,
) -> (RequestedServiceId, NodeId, bool) {
    // 1. Check for pending Exception Reset requests (highest priority).
    if let Some(node_to_reset) = context.pending_er_requests.pop() {
        info!("[MN] Prioritizing ER for Node {}.", node_to_reset.0);
        return (RequestedServiceId::StatusRequest, node_to_reset, true);
    }

    // 2. Check for pending StatusRequests due to error signaling.
    if let Some(node_to_query) = context.pending_status_requests.pop() {
        info!(
            "[MN] Prioritizing StatusRequest for Node {} due to error signal.",
            node_to_query.0
        );
        return (RequestedServiceId::StatusRequest, node_to_query, false);
    }

    // 3. Check for pending NMT commands generated by the MN.
    if !context.pending_nmt_commands.is_empty() {
        info!("[MN] Prioritizing internal NMT command for async slot.");
        return (
            RequestedServiceId::UnspecifiedInvite,
            NodeId(C_ADR_MN_DEF_NODE_ID),
            false,
        );
    }

    // 4. Check for pending generic async frames from the MN application.
    if !context.mn_async_send_queue.is_empty() {
        info!("[MN] Prioritizing MN-initiated generic async frame for async slot.");
        return (
            RequestedServiceId::UnspecifiedInvite,
            NodeId(C_ADR_MN_DEF_NODE_ID),
            false,
        );
    }

    // 5. Check for pending SDO client requests from the MN.
    if !context.pending_sdo_client_requests.is_empty() {
        info!("[MN] Prioritizing MN-initiated SDO request for async slot.");
        return (
            RequestedServiceId::UnspecifiedInvite,
            NodeId(C_ADR_MN_DEF_NODE_ID),
            false,
        );
    }

    // 6. Check for nodes to identify (next priority)
    if let Some(node_to_poll) = find_next_node_to_identify(context) {
        return (RequestedServiceId::IdentRequest, node_to_poll, false);
    }

    // 7. Check for async-only nodes to poll
    if let Some(node_to_poll) = find_next_async_only_to_poll(context) {
        return (RequestedServiceId::StatusRequest, node_to_poll, false);
    }

    // 8. Service pending ASnd requests from CNs (already in priority order from BinaryHeap)
    if let Some(request) = context.async_request_queue.pop() {
        info!(
            "[MN] Granting async slot to Node {} (PR={})",
            request.node_id.0, request.priority
        );
        let service_id = if request.priority == 7 {
            RequestedServiceId::NmtRequestInvite
        } else {
            RequestedServiceId::UnspecifiedInvite
        };
        return (service_id, request.node_id, false);
    }

    // 9. If nothing else to do, send a SoA with NoService
    (RequestedServiceId::NoService, NodeId(0), false)
}

/// Checks if MN can transition NMT state based on mandatory CN states.
pub(super) fn check_bootup_state(context: &mut MnContext) {
    let current_mn_state = context.nmt_state_machine.current_state();

    if current_mn_state == NmtState::NmtPreOperational1 {
        // Check if all mandatory nodes are Identified or further, but not Missing or Stopped
        let all_mandatory_identified = context.mandatory_nodes.iter().all(|node_id| {
            let state = context
                .node_info
                .get(node_id)
                .map_or(CnState::Unknown, |info| info.state);
            // Updated condition: >= Identified AND <= Operational
            state >= CnState::Identified && state <= CnState::Operational
        });

        if all_mandatory_identified {
            info!("[MN] All mandatory nodes identified. Triggering NMT transition to PreOp2.");
            // NMT_MT3
            context
                .nmt_state_machine
                .process_event(NmtEvent::AllCnsIdentified, &mut context.core.od);
        }
    } else if current_mn_state == NmtState::NmtPreOperational2 {
        // Check if all mandatory nodes are PreOperational or further (ReadyToOp reported via PRes/Status)
        let all_mandatory_preop = context.mandatory_nodes.iter().all(|node_id| {
            let state = context
                .node_info
                .get(node_id)
                .map_or(CnState::Unknown, |info| info.state);
            // CN reports PreOp2 or ReadyToOp, MN maps this to CnState::PreOperational
            // Also check <= Operational to ensure node hasn't gone missing/stopped
            state >= CnState::PreOperational && state <= CnState::Operational
        });

        if all_mandatory_preop {
            // Check MN startup flags if application trigger is needed
            // NMT_StartUp_U32.Bit8 = 0 -> Auto transition
            if context.nmt_state_machine.startup_flags & (1 << 8) == 0 {
                info!(
                    "[MN] All mandatory nodes PreOperational/ReadyToOp. Triggering NMT transition to ReadyToOp."
                );
                // NMT_MT4
                context
                    .nmt_state_machine
                    .process_event(NmtEvent::ConfigurationCompleteCnsReady, &mut context.core.od);
            } else {
                debug!(
                    "[MN] All mandatory nodes PreOperational/ReadyToOp, but waiting for application trigger to enter ReadyToOp."
                );
            }
        }
    } else if current_mn_state == NmtState::NmtReadyToOperate {
        // Check if all mandatory nodes are Operational
        let all_mandatory_operational = context.mandatory_nodes.iter().all(|node_id| {
            let state = context
                .node_info
                .get(node_id)
                .map_or(CnState::Unknown, |info| info.state);
            state >= CnState::Operational
        });
        if all_mandatory_operational {
            // Check MN startup flags if application trigger is needed
            // NMT_StartUp_U32.Bit2 = 0 -> Auto transition
            if context.nmt_state_machine.startup_flags & (1 << 2) == 0 {
                info!(
                    "[MN] All mandatory nodes Operational. Triggering NMT transition to Operational."
                );
                // NMT_MT5 - Use AllMandatoryCnsOperational event as the trigger
                context
                    .nmt_state_machine
                    .process_event(NmtEvent::AllMandatoryCnsOperational, &mut context.core.od);
            } else {
                debug!(
                    "[MN] All mandatory nodes Operational, but waiting for application trigger to enter Operational."
                );
            }
        }
    }
    // No automatic checks needed in Operational state based on CN states alone.
}

/// Finds the next configured CN that has not been identified yet for polling.
pub(super) fn find_next_node_to_identify(context: &mut MnContext) -> Option<NodeId> {
    // Start iterating from the node *after* the last one polled
    let start_node_id_val = context.last_ident_poll_node_id.0.wrapping_add(1); // Access pub(super) field

    let mut wrapped_around = false;
    let mut current_node_id_val = start_node_id_val;

    loop {
        // Handle wrap-around and node ID range (1-239 for CNs)
        if current_node_id_val == 0 || current_node_id_val > 239 {
            current_node_id_val = 1;
        }
        if current_node_id_val == start_node_id_val {
            if wrapped_around {
                debug!("[MN] Full circle check for unidentified nodes completed.");
                break; // Full circle, no nodes found
            }
            wrapped_around = true;
        }

        // Ensure NodeId::try_from is used or logic handles invalid IDs
        let node_id = NodeId(current_node_id_val); // Directly create NodeId

        // Check if this node ID exists in our configured node state map
        // AND if its current state is Unknown or Missing.
        let info = context.node_info.get(&node_id).copied();
        if matches!(
            info,
            Some(CnInfo {
                state: CnState::Unknown,
                ..
            }) | Some(CnInfo {
                state: CnState::Missing,
                ..
            })
        ) {
            // Found a node to poll
            debug!(
                "[MN] Found unidentified or missing Node {} to poll.",
                node_id.0
            );
            context.last_ident_poll_node_id = node_id; // Access pub(super) field
            return Some(node_id);
        }

        current_node_id_val = current_node_id_val.wrapping_add(1);
    }

    debug!("[MN] No more unidentified nodes found.");
    None // No unidentified nodes left
}

/// Finds the next async-only CN that needs a status poll.
pub(super) fn find_next_async_only_to_poll(context: &mut MnContext) -> Option<NodeId> {
    if context.async_only_nodes.is_empty() {
        return None;
    }

    // Find the index of the last polled node to continue from there
    let start_idx = context
        .async_only_nodes
        .iter()
        .position(|&id| id == context.last_status_poll_node_id)
        .map_or(0, |i| (i + 1) % context.async_only_nodes.len());

    // Iterate through the async-only list in a round-robin fashion
    for i in 0..context.async_only_nodes.len() {
        let current_idx = (start_idx + i) % context.async_only_nodes.len();
        let node_id = context.async_only_nodes[current_idx];

        // Poll any async-only node that is not considered completely gone
        if let Some(info) = context.node_info.get(&node_id) {
            if info.state != CnState::Missing {
                debug!(
                    "[MN] Found async-only Node {} to poll for status.",
                    node_id.0
                );
                context.last_status_poll_node_id = node_id;
                return Some(node_id);
            }
        }
    }

    None
}

/// Gets the Node ID of the next isochronous node to poll for the given multiplex cycle.
/// Returns None if all nodes for the current cycle have been polled.
/// This function modifies the internal `next_isoch_node_idx`.
pub(super) fn get_next_isochronous_node_to_poll(
    context: &mut MnContext,
    current_multiplex_cycle: u8,
) -> Option<NodeId> {
    // Iterate through the pre-defined list starting from the current index
    while context.next_isoch_node_idx < context.isochronous_nodes.len() {
        // Access pub(super) fields
        let node_id = context.isochronous_nodes[context.next_isoch_node_idx]; // Access pub(super) fields
        context.next_isoch_node_idx += 1; // Move to the next index for the *next* call // Access pub(super) field

        // Check if this node should be polled in the current multiplex cycle
        let assigned_cycle = context.multiplex_assign.get(&node_id).copied().unwrap_or(0);
        // Corrected multiplex check: cycle counter is 0-based, assigned is 1-based
        let should_poll_this_cycle = assigned_cycle == 0 // Continuous node
            || (context.multiplex_cycle_len > 0 && assigned_cycle == (current_multiplex_cycle + 1)); // Multiplexed node for this cycle (assigned cycle is 1-based)

        if should_poll_this_cycle {
            // Check if the node is in a state where it should be polled isochronously
            let state = context
                .node_info
                .get(&node_id)
                .map_or(CnState::Unknown, |info| info.state); // Access pub(super) field
            // Poll nodes from Identified onwards, excluding Stopped/Missing
            // PReq allowed in PreOp2, ReadyToOp, Operational
            // Corrected state check: >= PreOperational
            if state >= CnState::PreOperational {
                // Found a valid node to poll in this cycle
                trace!(
                    "[MN] Polling Node {} (State: {:?}, MuxCycle: {}) in mux cycle {}",
                    node_id.0, state, assigned_cycle, current_multiplex_cycle
                );
                return Some(node_id);
            } else {
                debug!(
                    "[MN] Skipping Node {} (State: {:?}) in isochronous polling for mux cycle {}.",
                    node_id.0, state, current_multiplex_cycle
                );
            }
        } else {
            trace!(
                "[MN] Skipping Node {} (assigned mux cycle {}) in current mux cycle {}.",
                node_id.0, assigned_cycle, current_multiplex_cycle
            );
        }
        // If the node is not in a pollable state or not for this cycle, the loop continues
    }
    None // No more nodes left to poll in this cycle
}

/// Helper to check if there are more isochronous nodes to poll in the current cycle.
/// Does not modify `next_isoch_node_idx`.
pub(super) fn has_more_isochronous_nodes(
    context: &MnContext,
    current_multiplex_cycle: u8,
) -> bool {
    // Check remaining nodes in the list from the current index
    for idx in context.next_isoch_node_idx..context.isochronous_nodes.len() {
        let node_id = context.isochronous_nodes[idx];
        let assigned_cycle = context.multiplex_assign.get(&node_id).copied().unwrap_or(0);
        // Corrected multiplex check
        let should_poll_this_cycle = assigned_cycle == 0
            || (context.multiplex_cycle_len > 0
                && assigned_cycle == (current_multiplex_cycle + 1));

        if should_poll_this_cycle {
            let state = context
                .node_info
                .get(&node_id)
                .map_or(CnState::Unknown, |info| info.state);
            // Corrected state check: >= PreOperational
            if state >= CnState::PreOperational {
                return true; // Found at least one more node to poll
            }
        }
    }
    false // No more pollable nodes found for this cycle
}

/// The MN's main scheduler tick.
/// This function was moved from `main.rs`.
pub(super) fn tick(context: &mut MnContext, current_time_us: u64) -> NodeAction {
    // --- 0. SDO Server Tick (handles timeouts/retransmissions) ---
    match context.core.sdo_server.tick(current_time_us, &context.core.od) {
        // `tick` now returns the components directly
        Ok(Some((client_info, seq_header, command))) => {
            #[cfg(feature = "sdo-udp")]
            let build_udp = || {
                // Pass components to build function
                build_udp_from_sdo_response(
                    context,
                    client_info, // No clone needed here
                    seq_header.clone(),
                    command.clone(),
                )
            };
            #[cfg(not(feature = "sdo-udp"))]
            let build_udp = || {
                Err::<NodeAction, PowerlinkError>(PowerlinkError::InternalError(
                    "UDP feature disabled",
                ))
            };

            match client_info {
                SdoClientInfo::Asnd { .. } => {
                    // Pass components to build function
                    match build_asnd_from_sdo_response(context, client_info, seq_header, command) {
                        Ok(action) => return action,
                        Err(e) => error!("[MN] Failed to build SDO Abort ASnd frame: {:?}", e),
                    }
                }
                #[cfg(feature = "sdo-udp")]
                SdoClientInfo::Udp { .. } => match build_udp() {
                    Ok(action) => return action,
                    Err(e) => error!("[MN] Failed to build SDO Abort UDP frame: {:?}", e),
                },
            }
        }
        Err(e) => error!("[MN] SDO Server tick error: {:?}", e),
        _ => {}
    }

    let current_nmt_state = context.nmt_state_machine.current_state();

    // --- 1. Handle one-time actions ---
    if current_nmt_state == NmtState::NmtOperational && !context.initial_operational_actions_done {
        context.initial_operational_actions_done = true;
        if (context.nmt_state_machine.startup_flags & (1 << 1)) != 0 {
            info!("[MN] Sending NMTStartNode (Broadcast).");
            return serialize_frame_action(                
                payload::build_nmt_command_frame(
                    context,
                    NmtCommand::StartNode,
                    NodeId(C_ADR_BROADCAST_NODE_ID),
                ),
                context
            ).unwrap_or(
                // TODO: Handle error properly
                NodeAction::NoAction
            );
        } else if let Some(&node_id) = context.mandatory_nodes.first() {
            info!("[MN] Queuing NMTStartNode (Unicast).");
            context
                .pending_nmt_commands
                .push((NmtCommand::StartNode, node_id));
        }
    } else if current_nmt_state < NmtState::NmtOperational {
        context.initial_operational_actions_done = false;
    }

    // --- 2. Handle immediate, non-time-based follow-up actions ---
    match context.current_phase {
        CyclePhase::AwaitingMnAsyncSend => {
            if let Some((command, target_node)) = context.pending_nmt_commands.pop() {
                info!(
                    "[MN] Sending queued NMT command {:?} for Node {}.",
                    command, target_node.0
                );
                context.current_phase = CyclePhase::Idle;
                return serialize_frame_action(
                    payload::build_nmt_command_frame(context, command, target_node),
                    context,
                ).unwrap_or(
                    // TODO: Handle error properly
                    NodeAction::NoAction 
                );
            } else if let Some(frame) = context.mn_async_send_queue.pop() {
                info!("[MN] Sending queued generic async frame.");
                context.current_phase = CyclePhase::Idle;
                return serialize_frame_action(frame, context).unwrap_or(
                    // TODO: handler error properly
                    NodeAction::NoAction
                );
            } else if let Some((target_node_id, sdo_payload)) =
                context.pending_sdo_client_requests.pop()
            {
                info!(
                    "[MN] Sending queued SDO client request to Node {}.",
                    target_node_id.0
                );
                context.current_phase = CyclePhase::Idle;
                let dest_mac = get_cn_mac_address(context, target_node_id).unwrap_or(
                    // Fallback to multicast, though this indicates a configuration error.
                    MacAddress(crate::types::C_DLL_MULTICAST_ASND),
                );
                let asnd = ASndFrame::new(
                    context.core.mac_address,
                    dest_mac,
                    target_node_id,
                    context.nmt_state_machine.node_id,
                    ServiceId::Sdo,
                    sdo_payload,
                );
                return serialize_frame_action(PowerlinkFrame::ASnd(asnd), context).unwrap_or(
                    // TODO: handle error properly
                    NodeAction::NoAction
                );
            } else {
                warn!("[MN] Was in AwaitingMnAsyncSend, but all send queues are empty.");
                context.current_phase = CyclePhase::Idle;
            }
        }
        CyclePhase::SoCSent => {
            return cycle::advance_cycle_phase(context, current_time_us);
        }
        _ => {}
    }

    // --- 3. Handle time-based actions ---
    if current_nmt_state == NmtState::NmtNotActive && context.next_tick_us.is_none() {
        let timeout_us = context.nmt_state_machine.wait_not_active_timeout as u64;
        context.next_tick_us = Some(current_time_us + timeout_us);
        return NodeAction::NoAction;
    }

    let deadline = context.next_tick_us.unwrap_or(u64::MAX);
    if current_time_us < deadline {
        return NodeAction::NoAction;
    }

    let mut action = NodeAction::NoAction;
    let mut schedule_next_cycle = true;

    if let Some(timeout_event) = context.pending_timeout_event.take() {
        let missed_node = context.current_polled_cn.unwrap_or(NodeId(0));
        warn!(
            "[MN] Timeout event {:?} for Node {}",
            timeout_event, missed_node.0
        );
        let dummy_frame = PowerlinkFrame::Soc(SocFrame::new(
            context.core.mac_address,
            Default::default(),
            NetTime {
                seconds: 0,
                nanoseconds: 0,
            },
            RelativeTime {
                seconds: 0,
                nanoseconds: 0,
            },
        ));
        events::handle_dll_event(context, timeout_event, &dummy_frame);

        if timeout_event == DllMsEvent::PresTimeout {
            if let Some(info) = context.node_info.get_mut(&missed_node) {
                if info.state >= CnState::Identified && info.state != CnState::Stopped {
                    info.state = CnState::Missing;
                }
            }
            action = cycle::advance_cycle_phase(context, current_time_us);
            schedule_next_cycle = false;
        } else if timeout_event == DllMsEvent::AsndTimeout {
            context.current_phase = CyclePhase::Idle;
        }
    } else if current_nmt_state == NmtState::NmtNotActive {
        info!("[MN] NotActive timeout expired. Proceeding to boot.");
        context
            .nmt_state_machine
            .process_event(NmtEvent::Timeout, &mut context.core.od);
    }

    if action == NodeAction::NoAction && context.current_phase == CyclePhase::Idle {
        context.current_cycle_start_time_us = current_time_us;
        debug!(
            "[MN] Tick: Cycle start at {}us (State: {:?})",
            current_time_us, current_nmt_state
        );

        if current_nmt_state >= NmtState::NmtPreOperational1 {
            context.dll_error_manager.on_cycle_complete();
            if current_nmt_state >= NmtState::NmtPreOperational2
                && context.multiplex_cycle_len > 0
            {
                context.current_multiplex_cycle =
                    (context.current_multiplex_cycle + 1) % context.multiplex_cycle_len;
            }
            action = match current_nmt_state {
                NmtState::NmtPreOperational1 => {
                    cycle::advance_cycle_phase(context, current_time_us)
                }
                _ => {
                    context.current_phase = CyclePhase::SoCSent;
                    context.next_isoch_node_idx = 0;
                    serialize_frame_action(
                        payload::build_soc_frame(
                            context,
                            context.current_multiplex_cycle,
                            context.multiplex_cycle_len,
                        ),
                        context
                    ).unwrap_or(
                        // TODO: handle error properly
                        NodeAction::NoAction
                    )
                }
            };
            schedule_next_cycle = false;
        }
    }

    if schedule_next_cycle {
        context.cycle_time_us = context.core.od.read_u32(OD_IDX_CYCLE_TIME, 0).unwrap_or(0) as u64;
        if context.cycle_time_us > 0 && current_nmt_state >= NmtState::NmtPreOperational1 {
            let base_time = if context.current_cycle_start_time_us > 0
                && current_time_us < context.current_cycle_start_time_us + context.cycle_time_us
            {
                context.current_cycle_start_time_us
            } else {
                current_time_us
            };
            let next_cycle_start = (base_time / context.cycle_time_us + 1) * context.cycle_time_us;
            if context.pending_timeout_event.is_none()
                || next_cycle_start <= context.next_tick_us.unwrap_or(u64::MAX)
            {
                context.next_tick_us = Some(next_cycle_start);
                debug!("[MN] Scheduling next cycle start at {}us", next_cycle_start);
            }
        }
    }
    action
}

//
// --- Helper functions moved from main.rs ---
//

/// Helper to potentially schedule a DLL timeout event.
pub(super) fn schedule_timeout(context: &mut MnContext, deadline_us: u64, event: DllMsEvent) {
    let next_event_time = context
        .next_tick_us
        .map_or(deadline_us, |next| deadline_us.min(next));
    if context.next_tick_us.is_none() || next_event_time < context.next_tick_us.unwrap() {
        context.next_tick_us = Some(next_event_time);
        if next_event_time == deadline_us {
            context.pending_timeout_event = Some(event);
            debug!("[MN] Scheduled {:?} timeout at {}us", event, deadline_us);
        } else {
            context.pending_timeout_event = None;
        }
    } else if next_event_time == deadline_us && context.pending_timeout_event.is_none() {
        context.pending_timeout_event = Some(event);
        debug!(
            "[MN] Scheduled {:?} timeout coinciding with next event at {}us",
            event, deadline_us
        );
    }
}

/// Gets a CN's MAC address from the Object Dictionary.
pub(super) fn get_cn_mac_address(context: &MnContext, node_id: NodeId) -> Option<MacAddress> {
    if let Some(Object::Array(entries)) = context.core.od.read_object(OD_IDX_MAC_MAP) {
        // Corrected: Read OD_IDX_MAC_MAP (0x1F84) which stores MACs as OctetString
        if let Some(ObjectValue::OctetString(mac_bytes)) = entries.get(node_id.0 as usize) {
            if mac_bytes.len() >= 6 && mac_bytes[0..6].iter().any(|&b| b != 0) {
                return Some(MacAddress(mac_bytes[0..6].try_into().unwrap()));
            }
        } else if let Some(ObjectValue::Unsigned32(mac_val_u32)) =
            entries.get(node_id.0 as usize)
        {
            // Fallback for old (incorrect) implementation that used U32
            warn!("Reading MAC address from U32, OD 0x1F84 should use OctetString.");
            let mac_bytes = mac_val_u32.to_le_bytes();
            if mac_bytes[0..6].iter().any(|&b| b != 0) {
                return Some(MacAddress(mac_bytes[0..6].try_into().unwrap()));
            }
        }
    }
    None
}

